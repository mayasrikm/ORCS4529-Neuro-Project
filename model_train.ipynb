{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import neurogym as ngym\n",
        "\n",
        "from tasks import AverageDirectionTest\n",
        "\n",
        "# Environment\n",
        "env = AverageDirectionTest()\n",
        "_ = env.reset()\n",
        "ob_size = env.observation_space.shape[0]\n",
        "act_size = env.action_space.n\n",
        "num_neurons = ob_size"
      ],
      "metadata": {
        "id": "LWxz6P5nQWiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBiuBbAIp-GI"
      },
      "outputs": [],
      "source": [
        "#Initialize variables\n",
        "alpha = 0.1\n",
        "phi = zeros((1, ob_size))          # (1, obs_size) denotes inputs from environment, zeros are placeholder for what the actual input values are\n",
        "w = np.random.randn((obs_size, 1))  # (obs_size, 1) weights used to learn importance of each input\n",
        "#create sparse array B\n",
        "B = np.zeros((num_neurons, obs_size)) #(num_neurons, num_inputs) represents weighted combination of what input values each neuron sees\n",
        "num_elements = num_neurons * obs_size\n",
        "sparsity = 0.2\n",
        "indices = np.random.choice(num_elements, num_elements*sparsity, replace=False)\n",
        "B.flat[indices] = np.random.randn(num_elements*sparsity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train loop\n",
        "for i in training_episodes:\n",
        "  V = phi * w\n",
        "  D =  B * V                      # (num_neurons, 1) list of decisions between left (0) and right (1)\n",
        "  r = env.get_reward_vals         # (num_neurons, 1) correct decisions -- get_reward_vals needs to return the correct decision given the environment state\n",
        "  delta = abs(D - r)              # (num_neurons, 1) error between decision and correct decision\n",
        "  w = w + alpha*(B.T*delta)       #w update"
      ],
      "metadata": {
        "id": "8Fbe2k6lQmTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}